{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"Social_Vulnerability_Index_2018_-_United_States__tract_20250119.csv\")\n",
    "\n",
    "# # Check the data overview\n",
    "print(\"Column names:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nSummary of 'RPL_THEMES' before preprocessing:\")\n",
    "print(df[\"RPL_THEMES\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep and copy data where 'RPL_THEMES' is between 0 and 1 \n",
    "df1 = df[df[\"RPL_THEMES\"].between(0, 1)].copy()\n",
    "\n",
    "print(\"\\nSummary of 'RPL_THEMES' (after filtering):\")\n",
    "print(df1[\"RPL_THEMES\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'RPL_THEMES_BIN'の分布:\n",
      "RPL_THEMES_BIN\n",
      "0    24060\n",
      "2    24057\n",
      "1    24056\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Divide 'RPL_THEMES' into three classes (low, medium, high) using tertiles \n",
    "df1['RPL_THEMES_BIN'] = pd.qcut(df1['RPL_THEMES'], q=3, labels=[0, 1, 2])\n",
    "\n",
    "# Check the distribution of the new classes\n",
    "print(\"\\nDistribution of 'RPL_THEMES_BIN':\")\n",
    "print(df1['RPL_THEMES_BIN'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "特徴量の先頭:\n",
      "                EP_POV          EP_UNEMP          EP_NOHSDP  \\\n",
      "24   8.500000000000000 8.900000000000000  2.000000000000000   \n",
      "107  7.800000000000000 5.600000000000000 11.699999999999999   \n",
      "198  8.000000000000000 2.600000000000000  8.699999999999999   \n",
      "211 11.400000000000000 4.700000000000000  1.300000000000000   \n",
      "233 17.899999999999999 2.100000000000000  8.800000000000001   \n",
      "\n",
      "             EP_MINRTY           EP_AGE65  \n",
      "24  20.300000000000001 11.199999999999999  \n",
      "107 33.899999999999999 22.100000000000001  \n",
      "198  1.700000000000000 21.000000000000000  \n",
      "211  3.500000000000000 20.600000000000001  \n",
      "233  4.200000000000000 29.699999999999999  \n",
      "\n",
      "目的変数の先頭:\n",
      "24     1\n",
      "107    2\n",
      "198    1\n",
      "211    0\n",
      "233    1\n",
      "Name: RPL_THEMES_BIN, dtype: int64\n",
      "\n",
      "欠損値の数:\n",
      "EP_POV       0\n",
      "EP_UNEMP     0\n",
      "EP_NOHSDP    0\n",
      "EP_MINRTY    0\n",
      "EP_AGE65     0\n",
      "dtype: int64\n",
      "\n",
      "欠損値の数（補完後）:\n",
      "EP_POV       0\n",
      "EP_UNEMP     0\n",
      "EP_NOHSDP    0\n",
      "EP_MINRTY    0\n",
      "EP_AGE65     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define target and feature variables \n",
    "target_variable = 'RPL_THEMES_BIN'\n",
    "feature_variables = ['EP_POV', 'EP_UNEMP', 'EP_NOHSDP', 'EP_MINRTY', 'EP_AGE65']\n",
    "X = df1[feature_variables]\n",
    "y = df1[target_variable].astype(int)  # Convert categorical to integer\n",
    "\n",
    "# Check the first few rows of feature variables\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "print(X.head())\n",
    "\n",
    "# Standardize the data\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "print(\"\\nFirst few rows of target variable:\")\n",
    "print(y.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nNumber of missing values:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Fill missing values with the median  \n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Check missing values again \n",
    "print(\"\\nNumber of missing values (after imputation):\")\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "連続変数を3ビンに分割したデータの先頭:\n",
      "             EP_POV          EP_UNEMP         EP_NOHSDP         EP_MINRTY  \\\n",
      "0 1.000000000000000 2.000000000000000 0.000000000000000 1.000000000000000   \n",
      "1 0.000000000000000 1.000000000000000 1.000000000000000 1.000000000000000   \n",
      "2 0.000000000000000 0.000000000000000 1.000000000000000 0.000000000000000   \n",
      "3 1.000000000000000 1.000000000000000 0.000000000000000 0.000000000000000   \n",
      "4 2.000000000000000 0.000000000000000 1.000000000000000 0.000000000000000   \n",
      "\n",
      "           EP_AGE65  \n",
      "0 0.000000000000000  \n",
      "1 2.000000000000000  \n",
      "2 2.000000000000000  \n",
      "3 2.000000000000000  \n",
      "4 2.000000000000000  \n"
     ]
    }
   ],
   "source": [
    "# Set float display to 8 decimal places  \n",
    "pd.options.display.float_format = '{:0.15f}'.format\n",
    "\n",
    "# Apply KBinsDiscretizer\n",
    "kbd = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "X_binned = kbd.fit_transform(X)\n",
    "X_binned = pd.DataFrame(X_binned, columns=feature_variables)\n",
    "\n",
    "print(\"\\nFirst few rows of data after binning into 3 bins:\")\n",
    "print(X_binned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "カイ二乗検定による特徴量重要度（トップ5）:\n",
      "     Feature        Chi2 Statistic           p-value\n",
      "2  EP_NOHSDP 26418.683296464521845 0.000000000000000\n",
      "0     EP_POV 26048.013400624549831 0.000000000000000\n",
      "3  EP_MINRTY 13713.022703648970491 0.000000000000000\n",
      "1   EP_UNEMP 13316.504565824296151 0.000000000000000\n",
      "4   EP_AGE65  2287.042858585005888 0.000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Perform Chi-square test \n",
    "chi2_stat, p_values = chi2(X_binned, y)\n",
    "\n",
    "# Create a DataFrame for statistics and p-values\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Feature': feature_variables,\n",
    "    'Chi2 Statistic': chi2_stat,\n",
    "    'p-value': p_values\n",
    "})\n",
    "\n",
    "# Sort by Chi-square statistic and get the top 5 \n",
    "chi2_top5 = chi2_results.sort_values(by='Chi2 Statistic', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on Chi-square test:\") \n",
    "print(chi2_top5[['Feature', 'Chi2 Statistic', 'p-value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "スピアマン相関による特徴量重要度（トップ5）:\n",
      "     Feature  Spearman Correlation           p-value\n",
      "2  EP_NOHSDP     0.777560089369926 0.000000000000000\n",
      "0     EP_POV     0.769867580747304 0.000000000000000\n",
      "1   EP_UNEMP     0.561995184353346 0.000000000000000\n",
      "3  EP_MINRTY     0.546526127253726 0.000000000000000\n",
      "4   EP_AGE65    -0.201356601681077 0.000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate feature importance using Spearman correlation\n",
    "spearman_results = []\n",
    "for feature in feature_variables:\n",
    "    corr, p = spearmanr(X[feature], y)\n",
    "    spearman_results.append({\n",
    "        'Feature': feature,\n",
    "        'Spearman Correlation': corr,  # Signed correlation \n",
    "        'p-value': round(p, 8)\n",
    "    })\n",
    "\n",
    "spearman_df = pd.DataFrame(spearman_results)\n",
    "\n",
    "# Sort by absolute correlation and get the top 5\n",
    "spearman_top5 = spearman_df.reindex(\n",
    "    spearman_df['Spearman Correlation'].abs().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 important features based on Spearman correlation:\")  \n",
    "print(spearman_top5[['Feature', 'Spearman Correlation', 'p-value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kendall's Tauによる特徴量重要度（トップ7）:\n",
      "     Feature        Kendall Tau           p-value\n",
      "2  EP_NOHSDP  0.638781154620952 0.000000000000000\n",
      "0     EP_POV  0.632952621889376 0.000000000000000\n",
      "1   EP_UNEMP  0.446652595041508 0.000000000000000\n",
      "3  EP_MINRTY  0.426741083447876 0.000000000000000\n",
      "4   EP_AGE65 -0.155548319323142 0.000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate feature importance using Kendall's Tau \n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "kendall_results = []\n",
    "\n",
    "for feature in feature_variables:\n",
    "\n",
    "    corr, p = kendalltau(X[feature], y)\n",
    "    kendall_results.append({\n",
    "        'Feature': feature,\n",
    "        'Kendall Tau': corr,\n",
    "        'p-value': round(p, 8)\n",
    "    })\n",
    "\n",
    "kendall_df = pd.DataFrame(kendall_results)\n",
    "\n",
    "# Sort by absolute value and get the top 5\n",
    "kendall_top5 = kendall_df.reindex(\n",
    "    kendall_df['Kendall Tau'].abs().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 important features based on Kendall's Tau:\") \n",
    "print(kendall_top5[['Feature', 'Kendall Tau', 'p-value']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest による特徴量重要度（トップ9）:\n",
      "EP_POV      0.298084056150380\n",
      "EP_NOHSDP   0.295770274360944\n",
      "EP_MINRTY   0.176786814801612\n",
      "EP_UNEMP    0.128568353794457\n",
      "EP_AGE65    0.100790500892607\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest model \n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances \n",
    "rf_importances = pd.Series(rf.feature_importances_, index=feature_variables)\n",
    "rf_top5 = rf_importances.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on Random Forest:\") \n",
    "print(rf_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiro/.pyenv/versions/3.9.16/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:15:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost による特徴量重要度（トップ9）:\n",
      "EP_POV      0.463402390480042\n",
      "EP_NOHSDP   0.352279692888260\n",
      "EP_MINRTY   0.090007804334164\n",
      "EP_UNEMP    0.059058733284473\n",
      "EP_AGE65    0.035251379013062\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Build and train XGBoost model\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb.fit(X, y)\n",
    "\n",
    "# Get feature importances  \n",
    "xgb_importances = pd.Series(xgb.feature_importances_, index=feature_variables)\n",
    "xgb_top5 = xgb_importances.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on XGBoost:\")  \n",
    "print(xgb_top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM による特徴量重要度（トップ9）:\n",
      "EP_NOHSDP   2.156098304974876\n",
      "EP_POV      1.815424765759417\n",
      "EP_UNEMP    0.635572181303814\n",
      "EP_MINRTY   0.597628749806063\n",
      "EP_AGE65    0.180090928876780\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build SVM model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create pipeline\n",
    "svc = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=42))\n",
    "svc.fit(X, y)\n",
    "\n",
    "# Get feature weights\n",
    "svc_weights = pd.Series(svc.named_steps['svc'].coef_[0], index=feature_variables)\n",
    "svc_top5 = svc_weights.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on SVM:\")\n",
    "print(svc_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso回帰による特徴量重要度（トップ9）:\n",
      "EP_NOHSDP   3.115316685555746\n",
      "EP_POV      2.406547358206410\n",
      "EP_UNEMP    0.919728992228639\n",
      "EP_MINRTY   0.867641134813764\n",
      "EP_AGE65    0.284622655895218\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build Lasso regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create pipeline \n",
    "lasso = make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', solver='liblinear', random_state=42))\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get feature weights\n",
    "lasso_weights = pd.Series(lasso.named_steps['logisticregression'].coef_[0], index=feature_variables)\n",
    "lasso_top5 = lasso_weights.abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 important features based on Lasso regression:\")\n",
    "print(lasso_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes による特徴量重要度（トップ9）:\n",
      "EP_NOHSDP   0.753779592307598\n",
      "EP_POV      0.745852869003636\n",
      "EP_MINRTY   0.588756756022015\n",
      "EP_UNEMP    0.550160000837189\n",
      "EP_AGE65    0.173679567560513\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create pipeline\n",
    "nb = make_pipeline(StandardScaler(), GaussianNB())\n",
    "nb.fit(X, y)\n",
    "\n",
    "# Get feature weights\n",
    "nb_weights = pd.Series(nb.named_steps['gaussiannb'].theta_[0], index=feature_variables)\n",
    "nb_top5 = nb_weights.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on Naive Bayes:\") \n",
    "print(nb_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "各手法による特徴量ランキング:\n",
      "   Rank Random Forest    XGBoost      Lasso        SVM Naive Bayes\n",
      "0     1        EP_POV     EP_POV  EP_NOHSDP  EP_NOHSDP   EP_NOHSDP\n",
      "1     2     EP_NOHSDP  EP_NOHSDP     EP_POV     EP_POV      EP_POV\n",
      "2     3     EP_MINRTY  EP_MINRTY   EP_UNEMP   EP_UNEMP   EP_MINRTY\n",
      "3     4      EP_UNEMP   EP_UNEMP  EP_MINRTY  EP_MINRTY    EP_UNEMP\n",
      "4     5      EP_AGE65   EP_AGE65   EP_AGE65   EP_AGE65    EP_AGE65\n"
     ]
    }
   ],
   "source": [
    "# Get top 5 features for each method in ranking order\n",
    "top_features_rf = rf_importances.sort_values(ascending=False).index.tolist()\n",
    "top_features_xgb = xgb_importances.sort_values(ascending=False).index.tolist()\n",
    "top_features_svc = svc_weights.abs().sort_values(ascending=False).index.tolist()\n",
    "top_features_lasso = lasso_weights.abs().sort_values(ascending=False).index.tolist()\n",
    "top_features_nb = nb_weights.abs().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Create a rank list from 1 to 5  \n",
    "rank = list(range(1, 6))\n",
    "\n",
    "# Create a combined DataFrame  \n",
    "df_rank = pd.DataFrame({\n",
    "    'Rank': rank,\n",
    "    'Random Forest': top_features_rf,\n",
    "    'XGBoost': top_features_xgb,\n",
    "    'Lasso': top_features_lasso,\n",
    "    'SVM': top_features_svc,\n",
    "    'Naive Bayes': top_features_nb\n",
    "})\n",
    "\n",
    "print(\"\\nFeature ranking by each method:\")  \n",
    "print(df_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "各手法のトップ5特徴量を統合した結果:\n",
      "     Random Forest Importance Chi-Squared Statistic Spearman Correlation\n",
      "Rank                                                                    \n",
      "1                      EP_POV             EP_NOHSDP            EP_NOHSDP\n",
      "2                   EP_NOHSDP                EP_POV               EP_POV\n",
      "3                   EP_MINRTY             EP_MINRTY             EP_UNEMP\n",
      "4                    EP_UNEMP              EP_UNEMP            EP_MINRTY\n",
      "5                    EP_AGE65              EP_AGE65             EP_AGE65\n"
     ]
    }
   ],
   "source": [
    "# Get top 5 features for each method in ranking order\n",
    "top_features_rf = rf_importances.sort_values(ascending=False).index.tolist()\n",
    "top_features_chi2 = chi2_top5['Feature'].tolist()\n",
    "top_features_spearman = spearman_top5['Feature'].tolist()\n",
    "\n",
    "# Create a rank list from 1 to 5 \n",
    "rank = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Get ranked features for each method \n",
    "features_rf_ranked = rf_importances.sort_values(ascending=False).index.tolist()\n",
    "features_chi2_ranked = chi2_top5['Feature'].tolist()\n",
    "features_spearman_ranked = spearman_top5['Feature'].tolist()\n",
    "\n",
    "# Create a combined DataFrame \n",
    "rank_table = pd.DataFrame({\n",
    "    'Rank': rank,\n",
    "    'Random Forest Importance': features_rf_ranked,\n",
    "    'Chi-Squared Statistic': features_chi2_ranked,\n",
    "    'Spearman Correlation': features_spearman_ranked\n",
    "})\n",
    "\n",
    "# Set index to Rank\n",
    "rank_table = rank_table.set_index('Rank')\n",
    "\n",
    "print(\"\\nCombined top 5 features from each method:\")  \n",
    "print(rank_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "項目\n",
    "* EP_POV → 貧困率\n",
    "* EP_NOHSDP → 高卒未満の割合\n",
    "* EP_UNEMP → 失業率\n",
    "* EP_MINRTY → 少数民族(白人、非ヒスパニック系を除くすべての人)\n",
    "* EP_AGE65 → 65歳以上の割合\n",
    "\n",
    "* RPL_THEMES → その地域の脆弱性を%で表す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "初回のVIF:\n",
      "     feature               VIF\n",
      "0     EP_POV 2.024807641178533\n",
      "2  EP_NOHSDP 1.915279483833580\n",
      "3  EP_MINRTY 1.840969557493327\n",
      "1   EP_UNEMP 1.613642996203697\n",
      "4   EP_AGE65 1.209273701564790\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# VIFの計算関数\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "# 最初のVIF計算\n",
    "vif_data = calculate_vif(X)\n",
    "print(\"\\n初回のVIF:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全ての説明変数がVIF < 5 なので、削除する必要はない。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
