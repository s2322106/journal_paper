{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "Index(['ST', 'STATE', 'ST_ABBR', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
      "       'E_TOTPOP', 'M_TOTPOP', 'E_HU',\n",
      "       ...\n",
      "       'F_NOVEH', 'F_GROUPQ', 'F_THEME4', 'F_TOTAL', 'E_UNINSUR', 'M_UNINSUR',\n",
      "       'EP_UNINSUR', 'MP_UNINSUR', 'E_DAYPOP', 'Shape'],\n",
      "      dtype='object', length=124)\n",
      "\n",
      "Summary of 'RPL_THEMES' before preprocessing:\n",
      "count    72837.000000\n",
      "mean        -8.611694\n",
      "std         94.996397\n",
      "min       -999.000000\n",
      "25%          0.243100\n",
      "50%          0.495400\n",
      "75%          0.747700\n",
      "max          1.000000\n",
      "Name: RPL_THEMES, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"Social_Vulnerability_Index_2018_-_United_States__tract_20250119.csv\")\n",
    "\n",
    "# Check the data overview\n",
    "print(\"Column names:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nSummary of 'RPL_THEMES' before preprocessing:\")\n",
    "print(df[\"RPL_THEMES\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of 'RPL_THEMES' (after filtering):\n",
      "count    72173.000000\n",
      "mean         0.499994\n",
      "std          0.288681\n",
      "min          0.000000\n",
      "25%          0.250000\n",
      "50%          0.500000\n",
      "75%          0.750000\n",
      "max          1.000000\n",
      "Name: RPL_THEMES, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Keep and copy data where 'RPL_THEMES' is between 0 and 1 \n",
    "df1 = df[df[\"RPL_THEMES\"].between(0, 1)].copy()\n",
    "\n",
    "print(\"\\nSummary of 'RPL_THEMES' (after filtering):\")\n",
    "print(df1[\"RPL_THEMES\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of 'RPL_THEMES_BIN':\n",
      "RPL_THEMES_BIN\n",
      "0    24060\n",
      "2    24057\n",
      "1    24056\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Divide 'RPL_THEMES' into three classes (low, medium, high) using tertiles \n",
    "df1['RPL_THEMES_BIN'] = pd.qcut(df1['RPL_THEMES'], q=3, labels=[0, 1, 2])\n",
    "\n",
    "# Check the distribution of the new classes\n",
    "print(\"\\nDistribution of 'RPL_THEMES_BIN':\")\n",
    "print(df1['RPL_THEMES_BIN'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of features:\n",
      "     EP_POV  EP_UNEMP  EP_NOHSDP  EP_MINRTY  EP_AGE65\n",
      "24      8.5       8.9        2.0       20.3      11.2\n",
      "107     7.8       5.6       11.7       33.9      22.1\n",
      "198     8.0       2.6        8.7        1.7      21.0\n",
      "211    11.4       4.7        1.3        3.5      20.6\n",
      "233    17.9       2.1        8.8        4.2      29.7\n",
      "\n",
      "First few rows of target variable:\n",
      "24     1\n",
      "107    2\n",
      "198    1\n",
      "211    0\n",
      "233    1\n",
      "Name: RPL_THEMES_BIN, dtype: int64\n",
      "\n",
      "Number of missing values:\n",
      "EP_POV       0\n",
      "EP_UNEMP     0\n",
      "EP_NOHSDP    0\n",
      "EP_MINRTY    0\n",
      "EP_AGE65     0\n",
      "dtype: int64\n",
      "\n",
      "Number of missing values (after imputation):\n",
      "EP_POV       0\n",
      "EP_UNEMP     0\n",
      "EP_NOHSDP    0\n",
      "EP_MINRTY    0\n",
      "EP_AGE65     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define target and feature variables \n",
    "target_variable = 'RPL_THEMES_BIN'\n",
    "feature_variables = ['EP_POV', 'EP_UNEMP', 'EP_NOHSDP', 'EP_MINRTY', 'EP_AGE65']\n",
    "X = df1[feature_variables]\n",
    "y = df1[target_variable].astype(int)  # Convert categorical to integer\n",
    "\n",
    "# Check the first few rows of feature variables\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "print(X.head())\n",
    "\n",
    "# Standardize the data\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "print(\"\\nFirst few rows of target variable:\")\n",
    "print(y.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nNumber of missing values:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Fill missing values with the median  \n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Check missing values again \n",
    "print(\"\\nNumber of missing values (after imputation):\")\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of data after binning into 3 bins:\n",
      "             EP_POV          EP_UNEMP         EP_NOHSDP         EP_MINRTY  \\\n",
      "0 1.000000000000000 2.000000000000000 0.000000000000000 1.000000000000000   \n",
      "1 0.000000000000000 1.000000000000000 1.000000000000000 1.000000000000000   \n",
      "2 0.000000000000000 0.000000000000000 1.000000000000000 0.000000000000000   \n",
      "3 1.000000000000000 1.000000000000000 0.000000000000000 0.000000000000000   \n",
      "4 2.000000000000000 0.000000000000000 1.000000000000000 0.000000000000000   \n",
      "\n",
      "           EP_AGE65  \n",
      "0 0.000000000000000  \n",
      "1 2.000000000000000  \n",
      "2 2.000000000000000  \n",
      "3 2.000000000000000  \n",
      "4 2.000000000000000  \n"
     ]
    }
   ],
   "source": [
    "# Set float display to 8 decimal places  \n",
    "pd.options.display.float_format = '{:0.15f}'.format\n",
    "\n",
    "# Apply KBinsDiscretizer\n",
    "kbd = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "X_binned = kbd.fit_transform(X)\n",
    "X_binned = pd.DataFrame(X_binned, columns=feature_variables)\n",
    "\n",
    "print(\"\\nFirst few rows of data after binning into 3 bins:\")\n",
    "print(X_binned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on Chi-square test:\n",
      "     Feature        Chi2 Statistic           p-value\n",
      "2  EP_NOHSDP 26418.683296464521845 0.000000000000000\n",
      "0     EP_POV 26048.013400624549831 0.000000000000000\n",
      "3  EP_MINRTY 13713.022703648970491 0.000000000000000\n",
      "1   EP_UNEMP 13316.504565824296151 0.000000000000000\n",
      "4   EP_AGE65  2287.042858585005888 0.000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Perform Chi-square test \n",
    "chi2_stat, p_values = chi2(X_binned, y)\n",
    "\n",
    "# Create a DataFrame for statistics and p-values\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Feature': feature_variables,\n",
    "    'Chi2 Statistic': chi2_stat,\n",
    "    'p-value': p_values\n",
    "})\n",
    "\n",
    "# Sort by Chi-square statistic and get the top 5 \n",
    "chi2_top5 = chi2_results.sort_values(by='Chi2 Statistic', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on Chi-square test:\") \n",
    "print(chi2_top5[['Feature', 'Chi2 Statistic', 'p-value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on Spearman correlation:\n",
      "     Feature  Spearman Correlation           p-value\n",
      "2  EP_NOHSDP     0.777560089369926 0.000000000000000\n",
      "0     EP_POV     0.769867580747304 0.000000000000000\n",
      "1   EP_UNEMP     0.561995184353346 0.000000000000000\n",
      "3  EP_MINRTY     0.546526127253726 0.000000000000000\n",
      "4   EP_AGE65    -0.201356601681077 0.000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate feature importance using Spearman correlation\n",
    "spearman_results = []\n",
    "for feature in feature_variables:\n",
    "    corr, p = spearmanr(X[feature], y)\n",
    "    spearman_results.append({\n",
    "        'Feature': feature,\n",
    "        'Spearman Correlation': corr,  # Signed correlation \n",
    "        'p-value': round(p, 8)\n",
    "    })\n",
    "\n",
    "spearman_df = pd.DataFrame(spearman_results)\n",
    "\n",
    "# Sort by absolute correlation and get the top 5\n",
    "spearman_top5 = spearman_df.reindex(\n",
    "    spearman_df['Spearman Correlation'].abs().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 important features based on Spearman correlation:\")  \n",
    "print(spearman_top5[['Feature', 'Spearman Correlation', 'p-value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on Kendall's Tau:\n",
      "     Feature        Kendall Tau           p-value\n",
      "2  EP_NOHSDP  0.638781154620952 0.000000000000000\n",
      "0     EP_POV  0.632952621889376 0.000000000000000\n",
      "1   EP_UNEMP  0.446652595041508 0.000000000000000\n",
      "3  EP_MINRTY  0.426741083447876 0.000000000000000\n",
      "4   EP_AGE65 -0.155548319323142 0.000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate feature importance using Kendall's Tau \n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "kendall_results = []\n",
    "\n",
    "for feature in feature_variables:\n",
    "\n",
    "    corr, p = kendalltau(X[feature], y)\n",
    "    kendall_results.append({\n",
    "        'Feature': feature,\n",
    "        'Kendall Tau': corr,\n",
    "        'p-value': round(p, 8)\n",
    "    })\n",
    "\n",
    "kendall_df = pd.DataFrame(kendall_results)\n",
    "\n",
    "# Sort by absolute value and get the top 5\n",
    "kendall_top5 = kendall_df.reindex(\n",
    "    kendall_df['Kendall Tau'].abs().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 important features based on Kendall's Tau:\") \n",
    "print(kendall_top5[['Feature', 'Kendall Tau', 'p-value']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on Random Forest:\n",
      "EP_POV      0.298084056150380\n",
      "EP_NOHSDP   0.295770274360944\n",
      "EP_MINRTY   0.176786814801612\n",
      "EP_UNEMP    0.128568353794457\n",
      "EP_AGE65    0.100790500892607\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest model \n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances \n",
    "rf_importances = pd.Series(rf.feature_importances_, index=feature_variables)\n",
    "rf_top5 = rf_importances.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on Random Forest:\") \n",
    "print(rf_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiro/.pyenv/versions/3.9.16/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:32:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on XGBoost:\n",
      "EP_POV      0.463402390480042\n",
      "EP_NOHSDP   0.352279692888260\n",
      "EP_MINRTY   0.090007804334164\n",
      "EP_UNEMP    0.059058733284473\n",
      "EP_AGE65    0.035251379013062\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Build and train XGBoost model\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb.fit(X, y)\n",
    "\n",
    "# Get feature importances  \n",
    "xgb_importances = pd.Series(xgb.feature_importances_, index=feature_variables)\n",
    "xgb_top5 = xgb_importances.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on XGBoost:\")  \n",
    "print(xgb_top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on SVM:\n",
      "EP_NOHSDP   2.156098304974876\n",
      "EP_POV      1.815424765759417\n",
      "EP_UNEMP    0.635572181303814\n",
      "EP_MINRTY   0.597628749806063\n",
      "EP_AGE65    0.180090928876780\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build SVM model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create pipeline\n",
    "svc = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=42))\n",
    "svc.fit(X, y)\n",
    "\n",
    "# Get feature weights\n",
    "svc_weights = pd.Series(svc.named_steps['svc'].coef_[0], index=feature_variables)\n",
    "svc_top5 = svc_weights.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on SVM:\")\n",
    "print(svc_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on Lasso regression:\n",
      "EP_NOHSDP   3.115316685555746\n",
      "EP_POV      2.406547358206410\n",
      "EP_UNEMP    0.919728992228639\n",
      "EP_MINRTY   0.867641134813764\n",
      "EP_AGE65    0.284622655895218\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build Lasso regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create pipeline \n",
    "lasso = make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', solver='liblinear', random_state=42))\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get feature weights\n",
    "lasso_weights = pd.Series(lasso.named_steps['logisticregression'].coef_[0], index=feature_variables)\n",
    "lasso_top5 = lasso_weights.abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 important features based on Lasso regression:\")\n",
    "print(lasso_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important features based on Naive Bayes:\n",
      "EP_NOHSDP   0.753779592307598\n",
      "EP_POV      0.745852869003636\n",
      "EP_MINRTY   0.588756756022015\n",
      "EP_UNEMP    0.550160000837189\n",
      "EP_AGE65    0.173679567560513\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create pipeline\n",
    "nb = make_pipeline(StandardScaler(), GaussianNB())\n",
    "nb.fit(X, y)\n",
    "\n",
    "# Get feature weights\n",
    "nb_weights = pd.Series(nb.named_steps['gaussiannb'].theta_[0], index=feature_variables)\n",
    "nb_top5 = nb_weights.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 important features based on Naive Bayes:\") \n",
    "print(nb_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature ranking by each method:\n",
      "   Rank Random Forest    XGBoost      Lasso        SVM Naive Bayes\n",
      "0     1        EP_POV     EP_POV  EP_NOHSDP  EP_NOHSDP   EP_NOHSDP\n",
      "1     2     EP_NOHSDP  EP_NOHSDP     EP_POV     EP_POV      EP_POV\n",
      "2     3     EP_MINRTY  EP_MINRTY   EP_UNEMP   EP_UNEMP   EP_MINRTY\n",
      "3     4      EP_UNEMP   EP_UNEMP  EP_MINRTY  EP_MINRTY    EP_UNEMP\n",
      "4     5      EP_AGE65   EP_AGE65   EP_AGE65   EP_AGE65    EP_AGE65\n"
     ]
    }
   ],
   "source": [
    "# Get top 5 features for each method in ranking order\n",
    "top_features_rf = rf_importances.sort_values(ascending=False).index.tolist()\n",
    "top_features_xgb = xgb_importances.sort_values(ascending=False).index.tolist()\n",
    "top_features_svc = svc_weights.abs().sort_values(ascending=False).index.tolist()\n",
    "top_features_lasso = lasso_weights.abs().sort_values(ascending=False).index.tolist()\n",
    "top_features_nb = nb_weights.abs().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Create a rank list from 1 to 5  \n",
    "rank = list(range(1, 6))\n",
    "\n",
    "# Create a combined DataFrame  \n",
    "df_rank = pd.DataFrame({\n",
    "    'Rank': rank,\n",
    "    'Random Forest': top_features_rf,\n",
    "    'XGBoost': top_features_xgb,\n",
    "    'Lasso': top_features_lasso,\n",
    "    'SVM': top_features_svc,\n",
    "    'Naive Bayes': top_features_nb\n",
    "})\n",
    "\n",
    "print(\"\\nFeature ranking by each method:\")  \n",
    "print(df_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined top 5 features from each method:\n",
      "     Chi-Squared Statistic Spearman Correlation Kendall Tau\n",
      "Rank                                                       \n",
      "1                EP_NOHSDP            EP_NOHSDP   EP_NOHSDP\n",
      "2                   EP_POV               EP_POV      EP_POV\n",
      "3                EP_MINRTY             EP_UNEMP    EP_UNEMP\n",
      "4                 EP_UNEMP            EP_MINRTY   EP_MINRTY\n",
      "5                 EP_AGE65             EP_AGE65    EP_AGE65\n"
     ]
    }
   ],
   "source": [
    "# Create a rank list from 1 to 5 \n",
    "rank = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Get ranked features for each method \n",
    "features_kendall_ranked = kendall_top5['Feature'].tolist()\n",
    "features_chi2_ranked = chi2_top5['Feature'].tolist()\n",
    "features_spearman_ranked = spearman_top5['Feature'].tolist()\n",
    "\n",
    "# Create a combined DataFrame \n",
    "rank_table = pd.DataFrame({\n",
    "    'Rank': rank,\n",
    "    'Chi-Squared Statistic': features_chi2_ranked,\n",
    "    'Spearman Correlation': features_spearman_ranked,\n",
    "    'Kendall Tau': features_kendall_ranked\n",
    "})\n",
    "\n",
    "# Set index to Rank\n",
    "rank_table = rank_table.set_index('Rank')\n",
    "\n",
    "print(\"\\nCombined top 5 features from each method:\")  \n",
    "print(rank_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Names\n",
    "* EP_POV → percentage of persons below the poverty estimate\n",
    "* EP_NOHSDP → percentage of persons with no high school diploma for those aged 25 and older\n",
    "* EP_UNEMP → unemployment rate estimate\n",
    "* EP_MINRTY → percentage minority estimate\n",
    "* EP_AGE65 → percentage of persons aged 65 and older estimate\n",
    "\n",
    "* RPL_THEMES → Overall percentile ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial VIF:\n",
      "     feature               VIF\n",
      "0     EP_POV 2.024807641178533\n",
      "2  EP_NOHSDP 1.915279483833580\n",
      "3  EP_MINRTY 1.840969557493327\n",
      "1   EP_UNEMP 1.613642996203697\n",
      "4   EP_AGE65 1.209273701564790\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# VIF calculation function \n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "# Initial VIF calculation \n",
    "vif_data = calculate_vif(X)\n",
    "print(\"\\nInitial VIF:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all explanatory variables have a VIF below 5, none require removal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
